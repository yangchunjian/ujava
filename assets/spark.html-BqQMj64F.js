import{_ as s,o as e,c as n,a as p}from"./app-B9GX8pnS.js";const l="/assets/img_85-CV0rciPz.png",r={};function t(i,a){return e(),n("div",null,a[0]||(a[0]=[p('<h2 id="什么是apache-spark" tabindex="-1"><a class="header-anchor" href="#什么是apache-spark"><span>什么是Apache Spark？</span></a></h2><p>答案：Apache Spark是一个快速、通用的集群计算系统，旨在处理大规模数据处理和分析任务。它提供了高级的编程模型和丰富的库，可以在分布式环境中进行数据处理、机器学习、图计算等。</p><h2 id="spark和hadoop有什么区别" tabindex="-1"><a class="header-anchor" href="#spark和hadoop有什么区别"><span>Spark和Hadoop有什么区别？</span></a></h2><p>答案：Spark和Hadoop都是用于大数据处理的框架，但有以下区别：</p><ul><li>数据处理模型：Spark提供了更灵活和高级的数据处理模型，如RDD（弹性分布式数据集）和DataFrame，而Hadoop使用的是基于MapReduce的批处理模型。</li><li>性能：由于Spark的内存计算和任务调度优化，它通常比Hadoop的MapReduce更快。</li><li>生态系统：Hadoop拥有更成熟和广泛的生态系统，包括HDFS、YARN和Hive等，而Spark在某些方面的生态系统仍在发展中。</li></ul><h2 id="spark的核心组件是什么" tabindex="-1"><a class="header-anchor" href="#spark的核心组件是什么"><span>Spark的核心组件是什么？</span></a></h2><p>答案：Spark的核心组件包括：</p><ul><li>Spark Core：提供了Spark的基本功能，包括任务调度、内存管理和分布式数据处理等。</li><li>Spark SQL：用于处理结构化数据的模块，支持SQL查询和DataFrame API。</li><li>Spark Streaming：用于实时数据流处理的模块，支持高吞吐量和低延迟的流处理任务。</li><li>MLlib：Spark的机器学习库，提供了常见的机器学习算法和工具。</li><li>GraphX：Spark的图计算库，用于图处理和分析任务。</li></ul><h2 id="spark的数据处理模型是什么" tabindex="-1"><a class="header-anchor" href="#spark的数据处理模型是什么"><span>Spark的数据处理模型是什么？</span></a></h2><p>答案：Spark的数据处理模型基于RDD（弹性分布式数据集）。RDD是一个可并行操作的、容错的、不可变的数据集合，可以在集群中进行分布式处理。Spark还引入了DataFrame和Dataset等高级抽象，提供了更丰富和优化的数据处理能力。</p><h2 id="spark支持哪些编程语言" tabindex="-1"><a class="header-anchor" href="#spark支持哪些编程语言"><span>Spark支持哪些编程语言？</span></a></h2><p>答案：Spark主要支持以下编程语言：</p><ul><li>Scala：Scala是Spark的主要编程语言，它是一种面向对象和函数式编程的语言，提供了强大的表达能力和丰富的特性。</li><li>Java：Spark可以通过Java API进行编程，适用于Java开发者。</li><li>Python：Spark提供了Python API（PySpark），可以使用Python语言进行Spark开发。</li><li>R：Spark也支持R语言，通过SparkR库可以在R环境中使用Spark。</li></ul><h2 id="spark的调度器是什么" tabindex="-1"><a class="header-anchor" href="#spark的调度器是什么"><span>Spark的调度器是什么？</span></a></h2><p>答案：Spark的调度器是负责将任务分配给集群中的执行节点的组件。Spark有两种调度器：</p><ul><li>FIFO调度器：按照任务提交的顺序依次执行，适用于简单的任务场景。</li><li>Fair调度器：根据资源使用情况动态分配资源，以保证每个任务能够公平地获得执行机会。</li></ul><h2 id="spark的数据持久化机制是什么" tabindex="-1"><a class="header-anchor" href="#spark的数据持久化机制是什么"><span>Spark的数据持久化机制是什么？</span></a></h2><p>答案：Spark使用RDD的持久化机制来将数据存储在内存或磁盘中，以加快后续操作的速度。它支持将RDD的数据持久化到内存、磁盘或者序列化到外部存储系统。</p><h2 id="spark的容错机制是什么" tabindex="-1"><a class="header-anchor" href="#spark的容错机制是什么"><span>Spark的容错机制是什么？</span></a></h2><p>答案：Spark的容错机制基于RDD的不可变性和记录操作的转换日志。当节点发生故障时，Spark可以根据转换日志重新计算丢失的数据，从而保证任务的容错性。</p><h2 id="spark的机器学习库是什么" tabindex="-1"><a class="header-anchor" href="#spark的机器学习库是什么"><span>Spark的机器学习库是什么？</span></a></h2><p>答案：Spark的机器学习库称为MLlib，它提供了常见的机器学习算法和工具，如分类、回归、聚类、推荐等。MLlib使用RDD和DataFrame作为数据接口，提供了易于使用和可扩展的机器学习功能。</p><h2 id="spark的图计算库是什么" tabindex="-1"><a class="header-anchor" href="#spark的图计算库是什么"><span>Spark的图计算库是什么？</span></a></h2><p>答案：Spark的图计算库称为GraphX，它提供了图处理和分析的功能，包括图的创建、遍历、连接等操作。GraphX使用RDD作为图数据的表示形式，并提供了丰富的图算法和操作符。</p><h2 id="spark支持哪些数据源和数据格式" tabindex="-1"><a class="header-anchor" href="#spark支持哪些数据源和数据格式"><span>Spark支持哪些数据源和数据格式？</span></a></h2><p>答案：Spark支持多种数据源和数据格式，包括：</p><ul><li>文件系统：Spark可以读写各种文件系统，如HDFS、本地文件系统、S3等。</li><li>数据库：Spark可以连接和读写关系型数据库，如MySQL、PostgreSQL等，也支持NoSQL数据库，如MongoDB、Cassandra等。</li><li>实时数据流：Spark支持读取实时数据流，如Kafka、Flume等。</li><li>数据格式：Spark支持常见的数据格式，如CSV、JSON、Parquet、Avro等。</li></ul><h2 id="spark的集群部署模式有哪些" tabindex="-1"><a class="header-anchor" href="#spark的集群部署模式有哪些"><span>Spark的集群部署模式有哪些？</span></a></h2><p>答案：Spark的集群部署模式包括：</p><ul><li>Standalone模式：在独立的Spark集群上运行，不依赖其他资源管理器。</li><li>YARN模式：在Hadoop集群上利用YARN资源管理器进行任务调度和资源分配。</li><li>Mesos模式：在Mesos集群上利用Mesos资源管理器进行任务调度和资源分配。</li><li>Kubernetes模式：在Kubernetes集群上运行Spark作业，利用Kubernetes进行容器管理和资源分配。</li></ul><h2 id="spark的优化技术有哪些" tabindex="-1"><a class="header-anchor" href="#spark的优化技术有哪些"><span>Spark的优化技术有哪些？</span></a></h2><p>答案：Spark的优化技术包括：</p><ul><li>延迟计算：Spark使用惰性求值策略，延迟计算数据，只在需要结果时才执行计算，减少不必要的中间结果。</li><li>数据分区和并行度：合理设置数据分区和并行度，使得任务可以并行执行，提高计算效率。</li><li>内存管理：通过合理配置内存和缓存策略，将常用的数据存储在内存中，减少磁盘IO，提高性能。</li><li>窗口操作：对于窗口操作，使用时间滑动窗口和窗口聚合等技术，减少数据的重复计算。</li><li>数据压缩和序列化：使用压缩和序列化技术减小数据的存储空间和传输成本。</li></ul><h2 id="spark支持哪些集群调度器" tabindex="-1"><a class="header-anchor" href="#spark支持哪些集群调度器"><span>Spark支持哪些集群调度器？</span></a></h2><p>答案：Spark支持以下集群调度器：</p><ul><li>Hadoop YARN：Spark可以与Hadoop集群上的YARN资源管理器集成，利用YARN进行任务调度和资源分配。</li><li>Apache Mesos：Spark可以与Mesos集群集成，利用Mesos进行任务调度和资源分配。</li><li>Kubernetes：Spark可以在Kubernetes集群上运行，利用Kubernetes进行容器管理和资源分配。</li></ul><h2 id="spark的数据处理模型中的rdd是什么" tabindex="-1"><a class="header-anchor" href="#spark的数据处理模型中的rdd是什么"><span>Spark的数据处理模型中的RDD是什么？</span></a></h2><p>答案：RDD（弹性分布式数据集）是Spark的核心数据结构，代表分布在集群中的不可变的、可分区的数据集合。RDD可以并行操作，支持容错和恢复。它是Spark进行分布式数据处理的基础。</p><h2 id="spark的dataframe是什么" tabindex="-1"><a class="header-anchor" href="#spark的dataframe是什么"><span>Spark的DataFrame是什么？</span></a></h2><p>答案：DataFrame是Spark提供的高级数据结构，用于处理结构化数据。DataFrame类似于传统数据库中的表格，具有列和行的概念，支持SQL查询和DataFrame API操作。DataFrame提供了更丰富的数据处理能力和优化技术，比原始的RDD更高效。</p><h2 id="spark的机器学习库mllib有哪些常见的算法" tabindex="-1"><a class="header-anchor" href="#spark的机器学习库mllib有哪些常见的算法"><span>Spark的机器学习库MLlib有哪些常见的算法？</span></a></h2><p>答案：Spark的机器学习库MLlib包含了多种常见的机器学习算法，包括分类算法（如逻辑回归、决策树、随机森林）、回归算法（如线性回归、岭回归）、聚类算法（如K均值聚类、高斯混合模型）、推荐算法（如协同过滤）、降维算法（如主成分分析）、自然语言处理（如文本分类、词嵌入）等。</p><h2 id="spark的图计算库graphx支持哪些图算法" tabindex="-1"><a class="header-anchor" href="#spark的图计算库graphx支持哪些图算法"><span>Spark的图计算库GraphX支持哪些图算法？</span></a></h2><p>答案：Spark的图计算库GraphX支持多种图算法，包括图的遍历、连接、聚合、PageRank、连通性组件、最短路径等。GraphX提供了丰富的图操作符和API，方便用户进行图处理和分析。</p><h2 id="spark-streaming是什么" tabindex="-1"><a class="header-anchor" href="#spark-streaming是什么"><span>Spark Streaming是什么？</span></a></h2><p>答案：Spark Streaming是Spark提供的实时数据流处理模块。它可以以微批处理的方式处理实时数据，支持高吞吐量和低延迟的数据处理。Spark Streaming可以与Spark的批处理和机器学习库无缝集成，实现批处理与实时处理的统一。</p><h2 id="spark的扩展库和整合工具有哪些" tabindex="-1"><a class="header-anchor" href="#spark的扩展库和整合工具有哪些"><span>Spark的扩展库和整合工具有哪些？</span></a></h2><p>答案：Spark有丰富的扩展库和整合工具，包括：</p><ul><li>Spark SQL：用于处理结构化数据，支持SQL查询和DataFrame操作。</li><li>Spark Streaming：用于实时数据流处理。</li><li>MLlib：Spark的机器学习库。</li><li>GraphX：Spark的图计算库。</li><li>SparkR：提供在R语言中使用Spark的能力。</li><li>PySpark：Spark的Python API。</li><li>Spark on Kubernetes：在Kubernetes集群上运行Spark作业。</li><li>Spark on Mesos：在Mesos集群上运行Spark作业。</li><li>Spark on YARN：在Hadoop YARN集群上运行Spark作业。</li></ul><h2 id="什么是宽依赖-什么是窄依赖-哪些算子是宽依赖-哪些是窄依赖" tabindex="-1"><a class="header-anchor" href="#什么是宽依赖-什么是窄依赖-哪些算子是宽依赖-哪些是窄依赖"><span>什么是宽依赖，什么是窄依赖？哪些算子是宽依赖，哪些是窄依赖？</span></a></h2><p>窄依赖就是一个父RDD分区对应一个子RDD分区，如map，filter<br> 或者多个父RDD分区对应一个子RDD分区，如co-partioned join</p><p>宽依赖是一个父RDD分区对应非全部的子RDD分区，如groupByKey，ruduceByKey<br> 或者一个父RDD分区对应全部的子RDD分区，如未经协同划分的join</p><h2 id="transformation和action算子有什么区别-举例说明" tabindex="-1"><a class="header-anchor" href="#transformation和action算子有什么区别-举例说明"><span>Transformation和action算子有什么区别？举例说明</span></a></h2><p>Transformation 变换/转换：这种变换并不触发提交作业，完成作业中间过程处理。Transformation 操作是延迟计算的，也就是说从一个RDD 转换生成另一个 RDD 的转换操作不是马上执行，需要等到有 Action 操作的时候才会真正触发运算</p><blockquote><p>map, filter</p></blockquote><p>Action 行动算子：这类算子会触发 SparkContext 提交 Job 作业。<br> Action 算子会触发 Spark 提交作业（Job）。</p><blockquote><p>count</p></blockquote><h2 id="讲解spark-shuffle原理和特性-shuffle-write-和-shuffle-read过程做些什么" tabindex="-1"><a class="header-anchor" href="#讲解spark-shuffle原理和特性-shuffle-write-和-shuffle-read过程做些什么"><span>讲解spark shuffle原理和特性？shuffle write 和 shuffle read过程做些什么？</span></a></h2><h2 id="shuffle数据块有多少种不同的存储方式-分别是什么" tabindex="-1"><a class="header-anchor" href="#shuffle数据块有多少种不同的存储方式-分别是什么"><span>Shuffle数据块有多少种不同的存储方式？分别是什么</span></a></h2><ol><li>RDD数据块：用来存储所缓存的RDD数据。</li><li>Shuffle数据块：用来存储持久化的Shuffle数据。</li><li>广播变量数据块：用来存储所存储的广播变量数据。</li><li>任务返回结果数据块：用来存储在存储管理模块内部的任务返回结果。通常情况下任务返回结果随任务一起通过Akka返回到Driver端。但是当任务返回结果很大时，会引起Akka帧溢出，这时的另一种方案是将返回结果以块的形式放入存储管理模块，然后在Driver端获取该数据块即可，因为存储管理模块内部数据块的传输是通过Socket连接的，因此就不会出现Akka帧溢出了。</li><li>流式数据块：只用在Spark Streaming中，用来存储所接收到的流式数据块</li></ol><h2 id="哪些spark算子会有shuffle" tabindex="-1"><a class="header-anchor" href="#哪些spark算子会有shuffle"><span>哪些spark算子会有shuffle？</span></a></h2><ol><li>去重，distinct</li><li>排序，groupByKey，reduceByKey等</li><li>重分区，repartition，coalesce</li><li>集合或者表操作，interection，join</li></ol><h2 id="讲解spark-schedule-任务调度" tabindex="-1"><a class="header-anchor" href="#讲解spark-schedule-任务调度"><span>讲解spark schedule（任务调度）？</span></a></h2><figure><img src="'+l+`" alt="img_85.png" tabindex="0" loading="lazy"><figcaption>img_85.png</figcaption></figure><h2 id="spark-stage是如何划分的" tabindex="-1"><a class="header-anchor" href="#spark-stage是如何划分的"><span>Spark stage是如何划分的？</span></a></h2><ol><li>从hdfs中读取文件后，创建 RDD 对象</li><li>DAGScheduler模块介入运算，计算RDD之间的依赖关系。RDD之间的依赖关系就形成了DAG</li><li>每一个JOB被分为多个Stage，划分Stage的一个主要依据是当前计算因子的输入是否是确定的，如果是则将其分在同一个Stage，避免多个Stage之间的消息传递开销。</li></ol><p>因此spark划分stage的整体思路是：从后往前推，遇到宽依赖就断开，划分为一个stage；遇到窄依赖就将这个RDD加入该stage中。</p><h2 id="spark-cache一定能提升计算性能么-说明原因" tabindex="-1"><a class="header-anchor" href="#spark-cache一定能提升计算性能么-说明原因"><span>Spark cache一定能提升计算性能么？说明原因？</span></a></h2><p>不一定啊，cache是将数据缓存到内存里，当小数据量的时候是能提升效率，但数据大的时候内存放不下就会报溢出。</p><h2 id="cache和persist有什么区别和联系" tabindex="-1"><a class="header-anchor" href="#cache和persist有什么区别和联系"><span>Cache和persist有什么区别和联系？</span></a></h2><p>cache调用了persist方法，cache只有一个默认的缓存级别MEMORY_ONLY ，而persist可以根据情况设置其它的缓存级别。</p><h2 id="rdd是弹性数据集-弹性-体现在哪里呢-你觉得rdd有哪些缺陷" tabindex="-1"><a class="header-anchor" href="#rdd是弹性数据集-弹性-体现在哪里呢-你觉得rdd有哪些缺陷"><span>RDD是弹性数据集，“弹性”体现在哪里呢？你觉得RDD有哪些缺陷？</span></a></h2><ol><li>自动进行内存和磁盘切换</li><li>基于lineage的高效容错</li><li>task如果失败会特定次数的重试</li><li>stage如果失败会自动进行特定次数的重试，而且只会只计算失败的分片</li><li>checkpoint【每次对RDD操作都会产生新的RDD，如果链条比较长，计算比较笨重，就把数据放在硬盘中】和persist 【内存或磁盘中对数据进行复用】(检查点、持久化)</li><li>数据调度弹性：DAG TASK 和资源管理无关</li><li>数据分片的高度弹性repartion</li></ol><p>缺陷：<br> 惰性计算的缺陷也是明显的：中间数据默认不会保存，每次动作操作都会对数据重复计算，某些计算量比较大的操作可能会影响到系统的运算效率</p><h2 id="rdd有多少种持久化方式-memory-only如果内存存储不了-会怎么操作" tabindex="-1"><a class="header-anchor" href="#rdd有多少种持久化方式-memory-only如果内存存储不了-会怎么操作"><span>RDD有多少种持久化方式？memory_only如果内存存储不了，会怎么操作？</span></a></h2><p>cache和persist</p><ul><li>memory_and_disk，放一部分到磁盘</li><li>MEMORY_ONLY_SER:同MEMORY_ONLY，但是会使用Java序列化方式，将Java对象序列化后进行持久化。可以减少内存开销，但是需要进行反序列化，因此会加大CPU开销。</li><li>MEMORY_AND_DSK_SER:同MEMORY_AND_DSK。但是使用序列化方式持久化Java对象。</li><li>DISK_ONLY:使用非序列化Java对象的方式持久化，完全存储到磁盘上。<br> MEMORY_ONLY_2或者MEMORY_AND_DISK_2等：如果是尾部加了2的持久化级别，表示会将持久化数据复用一份，保存到其他节点，从而在数据丢失时，不需要再次计算，只需要使用备份数据即可。</li></ul><h2 id="rdd分区和数据块有啥联系" tabindex="-1"><a class="header-anchor" href="#rdd分区和数据块有啥联系"><span>RDD分区和数据块有啥联系？</span></a></h2><h2 id="当gc时间占比很大可能的原因有哪些-对应的优化方法是" tabindex="-1"><a class="header-anchor" href="#当gc时间占比很大可能的原因有哪些-对应的优化方法是"><span>当GC时间占比很大可能的原因有哪些？对应的优化方法是？</span></a></h2><p>垃圾回收的开销和对象合数成正比，所以减少对象的个数，就能大大减少垃圾回收的开销。序列化存储数据，每个RDD就是一个对象。缓存RDD占用的内存可能跟工作所需的内存打架，需要控制好</p><h2 id="spark中repartition和coalesce异同-coalesce什么时候效果更高-为什么" tabindex="-1"><a class="header-anchor" href="#spark中repartition和coalesce异同-coalesce什么时候效果更高-为什么"><span>Spark中repartition和coalesce异同？coalesce什么时候效果更高，为什么?</span></a></h2><div class="language-less line-numbers-mode" data-ext="less" data-title="less"><pre class="language-less"><code><span class="token function">repartition</span><span class="token punctuation">(</span><span class="token property">numPartitions</span><span class="token punctuation">:</span>Int<span class="token punctuation">)</span><span class="token punctuation">:</span>RDD[T]
<span class="token function">coalesce</span><span class="token punctuation">(</span><span class="token property">numPartitions</span><span class="token punctuation">:</span>Int<span class="token punctuation">,</span> <span class="token property">shuffle</span><span class="token punctuation">:</span>Boolean=false<span class="token punctuation">)</span><span class="token punctuation">:</span>RDD[T]
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>以上为他们的定义，区别就是repartition一定会触发shuffle，而coalesce默认是不触发shuffle的。</p><p>他们两个都是RDD的分区进行重新划分，repartition只是coalesce接口中shuffle为true的简易实现，（假设RDD有N个分区，需要重新划分成M个分区）</p><p>减少分区提高效率</p><h2 id="groupbykey和reducebykey哪个性能更高-为什么" tabindex="-1"><a class="header-anchor" href="#groupbykey和reducebykey哪个性能更高-为什么"><span>Groupbykey和reducebykey哪个性能更高，为什么？</span></a></h2><p>reduceByKey性能高，更适合大数据集</p><h2 id="你是如何理解caseclass的" tabindex="-1"><a class="header-anchor" href="#你是如何理解caseclass的"><span>你是如何理解caseclass的？</span></a></h2><h2 id="scala里trait有什么功能-与class有何异同-什么时候用trait什么时候该用class" tabindex="-1"><a class="header-anchor" href="#scala里trait有什么功能-与class有何异同-什么时候用trait什么时候该用class"><span>Scala里trait有什么功能，与class有何异同？什么时候用trait什么时候该用class?</span></a></h2><p>它可以被继承，而且支持多重继承，其实它更像我们熟悉的接口（interface），但它与接口又有不同之处是：<br> trait中可以写方法的实现，interface不可以（java8开始支持接口中允许写方法实现代码了），这样看起来trait又很像抽象类</p><h2 id="scala-语法中to-和-until有啥区别" tabindex="-1"><a class="header-anchor" href="#scala-语法中to-和-until有啥区别"><span>Scala 语法中to 和 until有啥区别?</span></a></h2><p>to 包含上界，until不包含上界</p><h2 id="讲解scala伴生对象和伴生类" tabindex="-1"><a class="header-anchor" href="#讲解scala伴生对象和伴生类"><span>讲解Scala伴生对象和伴生类?</span></a></h2><p>单例对象与类同名时，这个单例对象被称为这个类的伴生对象，而这个类被称为这个单例对象的伴生类。伴生类和伴生对象要在同一个源文件中定义，伴生对象和伴生类可以互相访问其私有成员。不与伴生类同名的单例对象称为孤立对象。</p><div class="language-sql line-numbers-mode" data-ext="sql" data-title="sql"><pre class="language-sql"><code>
<span class="token keyword">import</span> scala<span class="token punctuation">.</span>collection<span class="token punctuation">.</span>mutable<span class="token punctuation">.</span>Map
 
class ChecksumAccumulator {
  private var sum <span class="token operator">=</span> <span class="token number">0</span>
  def <span class="token keyword">add</span><span class="token punctuation">(</span>b: Byte<span class="token punctuation">)</span> {
    sum <span class="token operator">+</span><span class="token operator">=</span> b
  }
  def checksum<span class="token punctuation">(</span><span class="token punctuation">)</span>: <span class="token keyword">Int</span> <span class="token operator">=</span> <span class="token operator">~</span><span class="token punctuation">(</span>sum <span class="token operator">&amp;</span> <span class="token number">0xFF</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span>
}
 
object ChecksumAccumulator {
  private val cache <span class="token operator">=</span> Map<span class="token punctuation">[</span>String<span class="token punctuation">,</span> <span class="token keyword">Int</span><span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
  def calculate<span class="token punctuation">(</span>s: String<span class="token punctuation">)</span>: <span class="token keyword">Int</span> <span class="token operator">=</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>cache<span class="token punctuation">.</span><span class="token keyword">contains</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">)</span>
    cache<span class="token punctuation">(</span>s<span class="token punctuation">)</span>
  <span class="token keyword">else</span> {
      val acc <span class="token operator">=</span> new ChecksumAccumulator
      <span class="token keyword">for</span> <span class="token punctuation">(</span>c <span class="token operator">&lt;</span><span class="token operator">-</span> s<span class="token punctuation">)</span>
        acc<span class="token punctuation">.</span><span class="token keyword">add</span><span class="token punctuation">(</span>c<span class="token punctuation">.</span>toByte<span class="token punctuation">)</span>
      val cs <span class="token operator">=</span> acc<span class="token punctuation">.</span>checksum<span class="token punctuation">(</span><span class="token punctuation">)</span>
      cache <span class="token operator">+</span><span class="token operator">=</span> <span class="token punctuation">(</span>s <span class="token operator">-</span><span class="token operator">&gt;</span> cs<span class="token punctuation">)</span>
      println<span class="token punctuation">(</span><span class="token string">&quot;s:&quot;</span><span class="token operator">+</span>s<span class="token operator">+</span><span class="token string">&quot; cs:&quot;</span><span class="token operator">+</span>cs<span class="token punctuation">)</span>
      cs
    }
 
  def main<span class="token punctuation">(</span>args: Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span> {
    println<span class="token punctuation">(</span><span class="token string">&quot;Java 1:&quot;</span><span class="token operator">+</span>calculate<span class="token punctuation">(</span><span class="token string">&quot;Java&quot;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    println<span class="token punctuation">(</span><span class="token string">&quot;Java 2:&quot;</span><span class="token operator">+</span>calculate<span class="token punctuation">(</span><span class="token string">&quot;Java&quot;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    println<span class="token punctuation">(</span><span class="token string">&quot;Scala :&quot;</span><span class="token operator">+</span>calculate<span class="token punctuation">(</span><span class="token string">&quot;Scala&quot;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
  }
}
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="spark作业执行流程" tabindex="-1"><a class="header-anchor" href="#spark作业执行流程"><span>spark作业执行流程?</span></a></h2><ol><li>客户端提交作业</li><li>Driver启动流程</li><li>Driver申请资源并启动其余Executor(即Container)</li><li>Executor启动流程</li><li>作业调度，生成stages与tasks。</li><li>Task调度到Executor上，Executor启动线程执行Task逻辑</li><li>Driver管理Task状态</li><li>Task完成，Stage完成，作业完成</li></ol>`,97)]))}const o=s(r,[["render",t],["__file","spark.html.vue"]]),k=JSON.parse('{"path":"/assembly/spark.html","title":"组件Spark","lang":"zh-CN","frontmatter":{"title":"组件Spark","icon":"laptop-code","category":["设计组件"],"tag":["组件"],"description":"什么是Apache Spark？ 答案：Apache Spark是一个快速、通用的集群计算系统，旨在处理大规模数据处理和分析任务。它提供了高级的编程模型和丰富的库，可以在分布式环境中进行数据处理、机器学习、图计算等。 Spark和Hadoop有什么区别？ 答案：Spark和Hadoop都是用于大数据处理的框架，但有以下区别： 数据处理模型：Spark提...","head":[["meta",{"property":"og:url","content":"https://ujava.cn/assembly/spark.html"}],["meta",{"property":"og:site_name","content":"UJava"}],["meta",{"property":"og:title","content":"组件Spark"}],["meta",{"property":"og:description","content":"什么是Apache Spark？ 答案：Apache Spark是一个快速、通用的集群计算系统，旨在处理大规模数据处理和分析任务。它提供了高级的编程模型和丰富的库，可以在分布式环境中进行数据处理、机器学习、图计算等。 Spark和Hadoop有什么区别？ 答案：Spark和Hadoop都是用于大数据处理的框架，但有以下区别： 数据处理模型：Spark提..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-04-27T02:43:03.000Z"}],["meta",{"property":"article:author","content":"UJava"}],["meta",{"property":"article:tag","content":"组件"}],["meta",{"property":"article:modified_time","content":"2024-04-27T02:43:03.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"组件Spark\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2024-04-27T02:43:03.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"UJava\\",\\"url\\":\\"https://ujava.cn/article/\\"}]}"]]},"headers":[{"level":2,"title":"什么是Apache Spark？","slug":"什么是apache-spark","link":"#什么是apache-spark","children":[]},{"level":2,"title":"Spark和Hadoop有什么区别？","slug":"spark和hadoop有什么区别","link":"#spark和hadoop有什么区别","children":[]},{"level":2,"title":"Spark的核心组件是什么？","slug":"spark的核心组件是什么","link":"#spark的核心组件是什么","children":[]},{"level":2,"title":"Spark的数据处理模型是什么？","slug":"spark的数据处理模型是什么","link":"#spark的数据处理模型是什么","children":[]},{"level":2,"title":"Spark支持哪些编程语言？","slug":"spark支持哪些编程语言","link":"#spark支持哪些编程语言","children":[]},{"level":2,"title":"Spark的调度器是什么？","slug":"spark的调度器是什么","link":"#spark的调度器是什么","children":[]},{"level":2,"title":"Spark的数据持久化机制是什么？","slug":"spark的数据持久化机制是什么","link":"#spark的数据持久化机制是什么","children":[]},{"level":2,"title":"Spark的容错机制是什么？","slug":"spark的容错机制是什么","link":"#spark的容错机制是什么","children":[]},{"level":2,"title":"Spark的机器学习库是什么？","slug":"spark的机器学习库是什么","link":"#spark的机器学习库是什么","children":[]},{"level":2,"title":"Spark的图计算库是什么？","slug":"spark的图计算库是什么","link":"#spark的图计算库是什么","children":[]},{"level":2,"title":"Spark支持哪些数据源和数据格式？","slug":"spark支持哪些数据源和数据格式","link":"#spark支持哪些数据源和数据格式","children":[]},{"level":2,"title":"Spark的集群部署模式有哪些？","slug":"spark的集群部署模式有哪些","link":"#spark的集群部署模式有哪些","children":[]},{"level":2,"title":"Spark的优化技术有哪些？","slug":"spark的优化技术有哪些","link":"#spark的优化技术有哪些","children":[]},{"level":2,"title":"Spark支持哪些集群调度器？","slug":"spark支持哪些集群调度器","link":"#spark支持哪些集群调度器","children":[]},{"level":2,"title":"Spark的数据处理模型中的RDD是什么？","slug":"spark的数据处理模型中的rdd是什么","link":"#spark的数据处理模型中的rdd是什么","children":[]},{"level":2,"title":"Spark的DataFrame是什么？","slug":"spark的dataframe是什么","link":"#spark的dataframe是什么","children":[]},{"level":2,"title":"Spark的机器学习库MLlib有哪些常见的算法？","slug":"spark的机器学习库mllib有哪些常见的算法","link":"#spark的机器学习库mllib有哪些常见的算法","children":[]},{"level":2,"title":"Spark的图计算库GraphX支持哪些图算法？","slug":"spark的图计算库graphx支持哪些图算法","link":"#spark的图计算库graphx支持哪些图算法","children":[]},{"level":2,"title":"Spark Streaming是什么？","slug":"spark-streaming是什么","link":"#spark-streaming是什么","children":[]},{"level":2,"title":"Spark的扩展库和整合工具有哪些？","slug":"spark的扩展库和整合工具有哪些","link":"#spark的扩展库和整合工具有哪些","children":[]},{"level":2,"title":"什么是宽依赖，什么是窄依赖？哪些算子是宽依赖，哪些是窄依赖？","slug":"什么是宽依赖-什么是窄依赖-哪些算子是宽依赖-哪些是窄依赖","link":"#什么是宽依赖-什么是窄依赖-哪些算子是宽依赖-哪些是窄依赖","children":[]},{"level":2,"title":"Transformation和action算子有什么区别？举例说明","slug":"transformation和action算子有什么区别-举例说明","link":"#transformation和action算子有什么区别-举例说明","children":[]},{"level":2,"title":"讲解spark shuffle原理和特性？shuffle write 和 shuffle read过程做些什么？","slug":"讲解spark-shuffle原理和特性-shuffle-write-和-shuffle-read过程做些什么","link":"#讲解spark-shuffle原理和特性-shuffle-write-和-shuffle-read过程做些什么","children":[]},{"level":2,"title":"Shuffle数据块有多少种不同的存储方式？分别是什么","slug":"shuffle数据块有多少种不同的存储方式-分别是什么","link":"#shuffle数据块有多少种不同的存储方式-分别是什么","children":[]},{"level":2,"title":"哪些spark算子会有shuffle？","slug":"哪些spark算子会有shuffle","link":"#哪些spark算子会有shuffle","children":[]},{"level":2,"title":"讲解spark schedule（任务调度）？","slug":"讲解spark-schedule-任务调度","link":"#讲解spark-schedule-任务调度","children":[]},{"level":2,"title":"Spark stage是如何划分的？","slug":"spark-stage是如何划分的","link":"#spark-stage是如何划分的","children":[]},{"level":2,"title":"Spark cache一定能提升计算性能么？说明原因？","slug":"spark-cache一定能提升计算性能么-说明原因","link":"#spark-cache一定能提升计算性能么-说明原因","children":[]},{"level":2,"title":"Cache和persist有什么区别和联系？","slug":"cache和persist有什么区别和联系","link":"#cache和persist有什么区别和联系","children":[]},{"level":2,"title":"RDD是弹性数据集，“弹性”体现在哪里呢？你觉得RDD有哪些缺陷？","slug":"rdd是弹性数据集-弹性-体现在哪里呢-你觉得rdd有哪些缺陷","link":"#rdd是弹性数据集-弹性-体现在哪里呢-你觉得rdd有哪些缺陷","children":[]},{"level":2,"title":"RDD有多少种持久化方式？memory_only如果内存存储不了，会怎么操作？","slug":"rdd有多少种持久化方式-memory-only如果内存存储不了-会怎么操作","link":"#rdd有多少种持久化方式-memory-only如果内存存储不了-会怎么操作","children":[]},{"level":2,"title":"RDD分区和数据块有啥联系？","slug":"rdd分区和数据块有啥联系","link":"#rdd分区和数据块有啥联系","children":[]},{"level":2,"title":"当GC时间占比很大可能的原因有哪些？对应的优化方法是？","slug":"当gc时间占比很大可能的原因有哪些-对应的优化方法是","link":"#当gc时间占比很大可能的原因有哪些-对应的优化方法是","children":[]},{"level":2,"title":"Spark中repartition和coalesce异同？coalesce什么时候效果更高，为什么?","slug":"spark中repartition和coalesce异同-coalesce什么时候效果更高-为什么","link":"#spark中repartition和coalesce异同-coalesce什么时候效果更高-为什么","children":[]},{"level":2,"title":"Groupbykey和reducebykey哪个性能更高，为什么？","slug":"groupbykey和reducebykey哪个性能更高-为什么","link":"#groupbykey和reducebykey哪个性能更高-为什么","children":[]},{"level":2,"title":"你是如何理解caseclass的？","slug":"你是如何理解caseclass的","link":"#你是如何理解caseclass的","children":[]},{"level":2,"title":"Scala里trait有什么功能，与class有何异同？什么时候用trait什么时候该用class?","slug":"scala里trait有什么功能-与class有何异同-什么时候用trait什么时候该用class","link":"#scala里trait有什么功能-与class有何异同-什么时候用trait什么时候该用class","children":[]},{"level":2,"title":"Scala 语法中to 和 until有啥区别?","slug":"scala-语法中to-和-until有啥区别","link":"#scala-语法中to-和-until有啥区别","children":[]},{"level":2,"title":"讲解Scala伴生对象和伴生类?","slug":"讲解scala伴生对象和伴生类","link":"#讲解scala伴生对象和伴生类","children":[]},{"level":2,"title":"spark作业执行流程?","slug":"spark作业执行流程","link":"#spark作业执行流程","children":[]}],"git":{"createdTime":1713599609000,"updatedTime":1714185783000,"contributors":[{"name":"yangchunjian","email":"1091938307@qq.com","commits":4}]},"readingTime":{"minutes":13.64,"words":4092},"filePathRelative":"assembly/spark.md","localizedDate":"2024年4月20日","excerpt":"<h2>什么是Apache Spark？</h2>\\n<p>答案：Apache Spark是一个快速、通用的集群计算系统，旨在处理大规模数据处理和分析任务。它提供了高级的编程模型和丰富的库，可以在分布式环境中进行数据处理、机器学习、图计算等。</p>\\n<h2>Spark和Hadoop有什么区别？</h2>\\n<p>答案：Spark和Hadoop都是用于大数据处理的框架，但有以下区别：</p>\\n<ul>\\n<li>数据处理模型：Spark提供了更灵活和高级的数据处理模型，如RDD（弹性分布式数据集）和DataFrame，而Hadoop使用的是基于MapReduce的批处理模型。</li>\\n<li>性能：由于Spark的内存计算和任务调度优化，它通常比Hadoop的MapReduce更快。</li>\\n<li>生态系统：Hadoop拥有更成熟和广泛的生态系统，包括HDFS、YARN和Hive等，而Spark在某些方面的生态系统仍在发展中。</li>\\n</ul>","autoDesc":true}');export{o as comp,k as data};
