import{_ as l,r as p,o,c as r,a as t,b as n,e as a,d as i}from"./app-xiTE5YBW.js";const c="/assets/img_111-DVchoEW0.png",d="/assets/img_112-BH8s6tNl.png",u="/assets/img_113-DbtBrGWo.png",m="/assets/img_114-CimtMFtD.png",g="/assets/img_115-BLJJiTUh.png",v="/assets/img_116-j4vpYIyK.png",h="/assets/img_117-nFFqt2S3.png",k="/assets/img_118-CnEqH6yc.png",b="/assets/img_119-D9NTfG7k.png",q="/assets/img_120-tBpf_k21.png",_="/assets/img_121-D-MusZIW.png",f="/assets/img_122-BomucrqL.png",y="/assets/img_123-CleYQ1lO.png",x="/assets/img_124-DzbgQLVs.png",E="/assets/img_125-ByAMiA0z.png",w="/assets/img_126-B-jpiGHs.png",S="/assets/img_127-DPc1sRVK.png",z="/assets/img_128-CP0IE-_S.png",D={},T={href:"https://blog.csdn.net/sinat_34814635/article/details/129914369",target:"_blank",rel:"noopener noreferrer"},L={href:"http://localhost:5601/app/dev_tools#/console",target:"_blank",rel:"noopener noreferrer"};function N(A,s){const e=p("ExternalLinkIcon");return o(),r("div",null,[s[4]||(s[4]=t('<h2 id="一、-简介" tabindex="-1"><a class="header-anchor" href="#一、-简介"><span>一、 简介</span></a></h2><p>Elasticsearch 是一个分布式可扩展的实时搜索和分析引擎,一个建立在全文搜索引擎 Apache Lucene™ 基础上的搜索引擎.当然 Elasticsearch 并不仅仅是 Lucene 那么简单，它不仅包括了全文搜索功能，还可以进行以下工作:</p><ul><li>一个分布式的实时文档存储，每个字段可以被索引与搜索</li><li>一个分布式实时分析搜索引擎</li><li>能胜任上百个服务节点的扩展，并支持 PB 级别的结构化或者非结构化数据</li></ul><h3 id="_1-es优点" tabindex="-1"><a class="header-anchor" href="#_1-es优点"><span>1.ES优点</span></a></h3><ul><li>1.支持全文搜索，并基于倒排索引，检索速度特别快</li><li>2.高可用性，支持集群部署，任何节点失效系统自动调整。</li></ul><h3 id="_2-es缺点" tabindex="-1"><a class="header-anchor" href="#_2-es缺点"><span>2.ES缺点</span></a></h3><ul><li>1.对硬件要求较高，es比较吃内存，需要存储资源来支持大量数据的处理。</li><li>2.不支持事务等ACID属性。</li></ul><h3 id="_3-es使用场景" tabindex="-1"><a class="header-anchor" href="#_3-es使用场景"><span>3.ES使用场景</span></a></h3><p>1.聊天消息</p><ul><li>数据量大，业务关系简单，会通过关键字搜索聊天信息</li><li>非核心业务，不会频繁crud</li><li>没有事务的场景，可以不用支持事务</li></ul><p>2.日志纪录</p><p>Elasticsearch 一个典型应用就是 ELK 日志分析系统。如nginx接入请求的访问日志纪录。</p><p>3.热点数据搜索</p><p>如电商业务的商品搜索等。</p><h3 id="_4-dsl语言高级查询" tabindex="-1"><a class="header-anchor" href="#_4-dsl语言高级查询"><span>4.DSL语言高级查询</span></a></h3><h4 id="_1-query-dsl概述" tabindex="-1"><a class="header-anchor" href="#_1-query-dsl概述"><span>1.Query DSL概述</span></a></h4><p>Domain Specific Language 领域专用语言</p><ul><li>Elasticsearch provides a ful1 Query DSL based on JSON to define queries</li><li>Elasticsearch提供了基于JSON的DSL来定义查询。</li></ul><p>DSL由叶子查询子句和复合查询子句两种子句组成。</p><figure><img src="'+c+`" alt="img_111.png" tabindex="0" loading="lazy"><figcaption>img_111.png</figcaption></figure><h4 id="_2-模糊匹配" tabindex="-1"><a class="header-anchor" href="#_2-模糊匹配"><span>2.模糊匹配</span></a></h4><p>模糊匹配主要是针对文本类型的字段，文本类型的字段会对内容进行分词，对查询时，也会对搜索条件进行分词，然后通过倒排索引查找到匹配的数据，模糊匹配主要通过match等参数来实现</p><ul><li>match : 通过match关键词模糊匹配条件内容</li><li>prefix : 前缀匹配</li><li>regexp : 通过正则表达式来匹配数据</li></ul><div class="language-sql line-numbers-mode" data-ext="sql" data-title="sql"><pre class="language-sql"><code>POST <span class="token operator">/</span>es_db<span class="token operator">/</span>_doc<span class="token operator">/</span>_search
{
<span class="token string">&quot;from&quot;</span>: <span class="token number">0</span><span class="token punctuation">,</span>
<span class="token string">&quot;size&quot;</span>: <span class="token number">2</span><span class="token punctuation">,</span>
<span class="token string">&quot;query&quot;</span>: {
<span class="token string">&quot;match&quot;</span>: {
<span class="token string">&quot;address&quot;</span>: <span class="token string">&quot;广州&quot;</span>
}
}
}
    
<span class="token keyword">SQL</span>: <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> <span class="token keyword">user</span> <span class="token keyword">where</span> address <span class="token operator">like</span> <span class="token string">&#39;%广州%&#39;</span> <span class="token keyword">limit</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_3-精确匹配" tabindex="-1"><a class="header-anchor" href="#_3-精确匹配"><span>3.精确匹配</span></a></h4><ul><li>term : 单个条件相等</li><li>terms : 单个字段属于某个值数组内的值</li><li>range : 字段属于某个范围内的值</li><li>exists : 某个字段的值是否存在</li><li>ids : 通过ID批量查询</li></ul><div class="language-sql line-numbers-mode" data-ext="sql" data-title="sql"><pre class="language-sql"><code>POST <span class="token operator">/</span>es_db<span class="token operator">/</span>_doc<span class="token operator">/</span>_search
{
<span class="token string">&quot;query&quot;</span>: {
<span class="token string">&quot;term&quot;</span>: {
<span class="token string">&quot;name&quot;</span>: <span class="token string">&quot;admin&quot;</span>
}
}
}

<span class="token keyword">SQL</span>: <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> student <span class="token keyword">where</span> name <span class="token operator">=</span> <span class="token string">&#39;admin&#39;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_4-聚合搜索" tabindex="-1"><a class="header-anchor" href="#_4-聚合搜索"><span>4.聚合搜索</span></a></h4><p><strong>1.bucket和metric</strong></p><p>bucket就是一个聚合搜索时的数据分组。</p><p>如：销售部门有员工张三和李四，开发部门有员工王五和赵六。那么根据部门分组聚合得到结果就是两个bucket。销售部门bucket中有张三和李四，开发部门 bucket中有王五和赵六。</p><p>metric就是对一个bucket数据执行的统计分析。如上述案例中，开发部门有2个员工，销售部门有2个员工，这就是metric。</p><p>metric有多种统计，如：求和，最大值，最小值，平均值等。</p><p>用一个大家容易理解的SQL语法来解释，如：select count() from table group by column。那么group by column分组后的每组数据就是bucket。对每个分组执行的count()就是metric。</p><p>es最重要的核心功能是数据检索，统计分析我认为不是es最核心的功能，想这种离线统计应该由其他的替代方案去做，所以如果想了解更多es聚合搜索相关知识可以参考官网或者其他博客</p><h3 id="_5-文档映射" tabindex="-1"><a class="header-anchor" href="#_5-文档映射"><span>5.文档映射</span></a></h3><p>ES中映射可以分为动态映射和静态映射</p><h4 id="_1-动态映射" tabindex="-1"><a class="header-anchor" href="#_1-动态映射"><span>1.动态映射</span></a></h4><p>在关系数据库中，需要事先创建数据库，然后在该数据库下创建数据表，并创建表字段、类型、长度、主键等，最后才能基于表插入数据。而Elasticsearch中不需要定义Mapping映射（即关系型数据库的表、字段等），在文档写入Elasticsearch时，会根据文档字段自动识别类型，这种机制称之为动态映射。<br> 动态映射规则如下：</p><figure><img src="`+d+`" alt="img_112.png" tabindex="0" loading="lazy"><figcaption>img_112.png</figcaption></figure><h4 id="_2-静态映射" tabindex="-1"><a class="header-anchor" href="#_2-静态映射"><span>2.静态映射</span></a></h4><p>静态映射是在Elasticsearch中也可以事先定义好映射，包含文档的各字段类型、分词器等，这种方式称之为静态映射。</p><h4 id="_3-核心类型-core-datatype" tabindex="-1"><a class="header-anchor" href="#_3-核心类型-core-datatype"><span>3.核心类型（Core datatype）</span></a></h4><ul><li>字符串：string，string类型包含 text 和 keyword。</li><li>text：该类型被用来索引长文本，在创建索引前会将这些文本进行分词，转化为词的组合，建立索引；允许es来检索这些词，text类型不能用来排序和聚合。</li><li>keyword：该类型不能分词，可以被用来检索过滤、排序和聚合，keyword类型不可用text进行分词模糊检索。</li><li>数值型：long、integer、short、byte、double、float</li><li>日期型：date</li><li>布尔型：boolean</li></ul><h4 id="_4-数据建模" tabindex="-1"><a class="header-anchor" href="#_4-数据建模"><span>4.数据建模</span></a></h4><p>就是针对于关系型数据库的一对多数据模型，而我自己认为es的主要应用场景是全文搜索引擎，这种复杂的业务关系就应该由关系型数据库如mysql去完成数据建模和存储，而不是交给es去建模存储，当然es也是提供了Parent / Child相关机制继续数据建模，如果有场景使用到，自己去参考对应的官方文档了解即可，但我自己不建议这样做。</p><h4 id="_5-分页查询" tabindex="-1"><a class="header-anchor" href="#_5-分页查询"><span>5.分页查询</span></a></h4><p>1.语法</p><p>在存在大量数据时，一般我们进行查询都需要进行分页查询。例如：我们指定页码、并指定每页显示多少条数据，然后Elasticsearch返回对应页码的数据。</p><p>在执行查询时，可以指定from（从第几条数据开始查起）和size（每页返回多少条）数据，就可以轻松完成分页。</p><div class="language-sql line-numbers-mode" data-ext="sql" data-title="sql"><pre class="language-sql"><code>POST <span class="token operator">/</span>es_db<span class="token operator">/</span>_doc<span class="token operator">/</span>_search
{
<span class="token string">&quot;from&quot;</span>: <span class="token number">0</span><span class="token punctuation">,</span>
<span class="token string">&quot;size&quot;</span>: <span class="token number">2</span><span class="token punctuation">,</span>
<span class="token string">&quot;query&quot;</span>: {
<span class="token string">&quot;match&quot;</span>: {
<span class="token string">&quot;address&quot;</span>: <span class="token string">&quot;广州天河&quot;</span>
}
}
}
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>2.scroll解决深分页问题</p><p>前面使用from和size方式，查询在1W条数据以内都是OK的，但如果数据比较多的时候，会出现性能问题。Elasticsearch做了一个限制，不允许查询的是10000条以后的数据。如果要查询1W条以后的数据，需要使用Elasticsearch中提供的scroll游标来查询。</p><p>在进行大量分页时，每次分页都需要将要查询的数据进行重新排序，这样非常浪费性能。</p><p>使用scroll是将要用的数据一次性排序好，然后分批取出,性能要比from + size好得多。</p><p>使用scroll查询后，排序后的数据会保持一定的时间，后续的分页查询都从该快照取数据即可。</p><p>第一次使用scroll分页查询</p><p>此处，我们让排序的数据保持1分钟，所以设置scroll为1m</p><div class="language-sql line-numbers-mode" data-ext="sql" data-title="sql"><pre class="language-sql"><code>GET <span class="token operator">/</span>es_db<span class="token operator">/</span>_search?scroll<span class="token operator">=</span><span class="token number">1</span>m
{
<span class="token string">&quot;query&quot;</span>: {
<span class="token string">&quot;multi_match&quot;</span>:{
<span class="token string">&quot;query&quot;</span>:<span class="token string">&quot;广州长沙张三&quot;</span><span class="token punctuation">,</span>
<span class="token string">&quot;fields&quot;</span>:<span class="token punctuation">[</span><span class="token string">&quot;address&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;name&quot;</span><span class="token punctuation">]</span>
}
}<span class="token punctuation">,</span>
<span class="token string">&quot;size&quot;</span>:<span class="token number">100</span>
}
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>执行后，我们注意到，在响应结果中有一项：</p><div class="language-sql line-numbers-mode" data-ext="sql" data-title="sql"><pre class="language-sql"><code><span class="token string">&quot;_scroll_id&quot;</span>: <span class="token string">&quot;DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAZEWY2VQZXBia1JTVkdhTWkwSl9GaUYtQQ==&quot;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>后续，我们需要根据这个_scroll_id来进行查询</p><p>第二次直接使用scroll id进行查询</p><h3 id="_6-suggest-search" tabindex="-1"><a class="header-anchor" href="#_6-suggest-search"><span>6.suggest search</span></a></h3><p>suggest search（completion suggest）：就是建议搜索或称为搜索建议，也可以叫做自动完成-auto completion。类似百度中的搜索联想提示功能。</p><p>ES实现suggest的时候，性能非常高，其构建的不是倒排索引，也不是正排索引，就是纯的用于进行前缀搜索的一种特殊的数据结构，而且会全部放在内存中，所以suggest search进行的前缀搜索提示，性能是非常高。</p><p>需要使用suggest的时候，必须在定义index时，为其mapping指定开启suggest。</p><p>具体如下:</p><div class="language-sql line-numbers-mode" data-ext="sql" data-title="sql"><pre class="language-sql"><code>PUT <span class="token operator">/</span>movie
{
<span class="token string">&quot;mappings&quot;</span>: {
<span class="token string">&quot;properties&quot;</span> : {
<span class="token string">&quot;title&quot;</span> : {
<span class="token string">&quot;type&quot;</span>: <span class="token string">&quot;text&quot;</span><span class="token punctuation">,</span>
<span class="token string">&quot;analyzer&quot;</span>: <span class="token string">&quot;ik_max_word&quot;</span><span class="token punctuation">,</span>
<span class="token string">&quot;fields&quot;</span>: {
<span class="token string">&quot;suggest&quot;</span> : {
<span class="token string">&quot;type&quot;</span> : <span class="token string">&quot;completion&quot;</span><span class="token punctuation">,</span>
<span class="token string">&quot;analyzer&quot;</span>: <span class="token string">&quot;ik_max_word&quot;</span>
}
}
}<span class="token punctuation">,</span>
<span class="token string">&quot;content&quot;</span>: {
<span class="token string">&quot;type&quot;</span>: <span class="token string">&quot;text&quot;</span><span class="token punctuation">,</span>
<span class="token string">&quot;analyzer&quot;</span>: <span class="token string">&quot;ik_max_word&quot;</span>
}
}
}
}

PUT <span class="token operator">/</span>movie<span class="token operator">/</span>_doc<span class="token operator">/</span><span class="token number">1</span>
{
<span class="token string">&quot;title&quot;</span>: <span class="token string">&quot;西游记电影系列&quot;</span><span class="token punctuation">,</span>
<span class="token string">&quot;content&quot;</span>: <span class="token string">&quot;西游记之月光宝盒将与2021年进行......&quot;</span>
}

PUT <span class="token operator">/</span>movie<span class="token operator">/</span>_doc<span class="token operator">/</span><span class="token number">2</span>
{
<span class="token string">&quot;title&quot;</span>: <span class="token string">&quot;西游记文学系列&quot;</span><span class="token punctuation">,</span>
<span class="token string">&quot;content&quot;</span>: <span class="token string">&quot;某知名网络小说作家已经完成了大话西游同名小说的出版&quot;</span>
}

PUT <span class="token operator">/</span>movie<span class="token operator">/</span>_doc<span class="token operator">/</span><span class="token number">3</span>
{
<span class="token string">&quot;title&quot;</span>: <span class="token string">&quot;西游记之大话西游手游&quot;</span><span class="token punctuation">,</span>
<span class="token string">&quot;content&quot;</span>: <span class="token string">&quot;网易游戏近日出品了大话西游经典IP的手游，正在火爆内测中&quot;</span>
}

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>suggest 搜索：</p><div class="language-sql line-numbers-mode" data-ext="sql" data-title="sql"><pre class="language-sql"><code>GET <span class="token operator">/</span>movie<span class="token operator">/</span>_search
{
<span class="token string">&quot;suggest&quot;</span>: {
<span class="token string">&quot;my-suggest&quot;</span> : {
<span class="token string">&quot;prefix&quot;</span> : <span class="token string">&quot;西游记&quot;</span><span class="token punctuation">,</span>
<span class="token string">&quot;completion&quot;</span> : {
<span class="token string">&quot;field&quot;</span> : <span class="token string">&quot;title.suggest&quot;</span>
}
}
}
}
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="二、es核心原理" tabindex="-1"><a class="header-anchor" href="#二、es核心原理"><span>二、ES核心原理</span></a></h2><p>ES存储模型</p><p>Elasticsearch与关系数据库结构对应</p><figure><img src="`+u+'" alt="img_113.png" tabindex="0" loading="lazy"><figcaption>img_113.png</figcaption></figure><p>ElasticSearch的对象模型，跟关系型数据库模型相比：</p><h3 id="_1-索引-index" tabindex="-1"><a class="header-anchor" href="#_1-索引-index"><span>1.索引（Index）</span></a></h3><p>相当于数据库，用于定义文档类型的存储；在同一个索引中，同一个字段只能定义一个数据类型；</p><p>一个索引就是一个拥有几分相似特征的文档的集合。比如说，可以有一个客户数据的索引，另一个产品目录的索引，还有一个订单数据的索引<br> 一个索引由一个名字来标识（必须全部是小写字母的），并且当我们要对对应于这个索引中的文档进行索引、搜索、更新和删除的时候，都要使用到这个名字</p><h3 id="_2-文档类型-type" tabindex="-1"><a class="header-anchor" href="#_2-文档类型-type"><span>2.文档类型（Type）</span></a></h3><p>相当于关系表，用于描述文档中的各个字段的定义；不同的文档类型，能够存储不同的字段，服务于不同的查询请求；</p><p>每一个字段都应该有一个对应的类型，例如：Text、Keyword、Byte等</p><h3 id="_3-文档-document" tabindex="-1"><a class="header-anchor" href="#_3-文档-document"><span>3.文档（Document）</span></a></h3><p>相当于关系表的数据行，存储数据的载体，包含一个或多个存有数据的字段；</p><p>一个文档是一个可被索引的基础信息单元，类似一条记录。文档以JSON（Javascript Object Notation）格式来表示；</p><h3 id="_4-字段-field" tabindex="-1"><a class="header-anchor" href="#_4-字段-field"><span>4.字段（Field）</span></a></h3><p>文档的一个Key/Value对；</p><ul><li>词（Term）：表示文本中的一个单词；</li><li>标记（Token）：表示在字段中出现的词，由该词的文本、偏移量（开始和结束）以及类型组成；</li></ul><p>相当于是数据表的字段|列</p><h3 id="_5-倒排索引" tabindex="-1"><a class="header-anchor" href="#_5-倒排索引"><span>5.倒排索引</span></a></h3><p>全文搜索引擎的技术原理被称为“倒排索引”（Inverted index），也常被称为反向索引、置入档案或反向档案，是一种索引方法，其基本原理是建立单词到文档的索引。</p><p>之所以被称为“倒排”索引，是和“正排“索引相对的，“正排索引”的基本原理是建立文档到单词的索引。我们通过一个简单的样例来说明这两种索引的差异。</p><p>假设我们有一个技术文章的网站，里面收集了各种技术文章，用户可以在网站浏览或者搜索文章。</p><p>正排索引示例：</p><figure><img src="'+m+'" alt="img_114.png" tabindex="0" loading="lazy"><figcaption>img_114.png</figcaption></figure><p>（注：文章内容仅为示范，文章内容实际上存储的是几千字的内容。）</p><p>正排索引适用于根据文档名称来查询文档内容。例如，用户在网站上单击了“面向对象葵花宝典是什么”，网站根据文章标题查询文章的内容展示给用户。</p><p>倒排索引示例：</p><figure><img src="'+g+'" alt="img_115.png" tabindex="0" loading="lazy"><figcaption>img_115.png</figcaption></figure><p>（注：表格仅为示范，不是完整的倒排索引表格，实际上的倒排索引有成千上万行，因为每个单词就是一个索引。）</p><p>倒排索引适用于根据关键词来查询文档内容，它是根据文章内容中的关键字建立索引，而值对应于文档ID，而搜索出来的结果就是文档ID所在行的所有内容。</p><p>例如，用户只是想看“设计”相关的文章，网站需要将文章内容中包含“设计”一词的文章都搜索出来展示给用户。</p><p>要注意倒排索引的两个重要细节：</p><ul><li>倒排索引中的所有词项对应一个或多个文档；</li><li>倒排索引中的词项根据字典顺序升序排列<br> Elasticsearch 也是 Master-slave 架构，也实现了数据的分片和备份。</li></ul><h3 id="_6-分词器" tabindex="-1"><a class="header-anchor" href="#_6-分词器"><span>6.分词器</span></a></h3><p>对于英文来说，分词比较简单，只需要按照单词的空格来进行分词。<br> 如下所示<br><img src="'+v+`" alt="img_116.png" loading="lazy"></p><p>如果要搜索hello这个关键词，则匹配的结果将是下面两个内容</p><div class="language-sql line-numbers-mode" data-ext="sql" data-title="sql"><pre class="language-sql"><code>hello world
hello elasticsearch

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>ES的默认分词设置是standard，这个在中文分词时就比较尴尬了，会单字拆分，比如我搜索关键词“清华大学”，这时候会按“清”，“华”，“大”，“学”去分词，然后搜出来的都是些“清清的河水”，“中华儿女”，“地大物博”，“学而不思则罔”之类的莫名其妙的结果。</p><p>这里我们就想把这个分词方式修改一下，于是呢，就想到了ik分词器，有两种ik_smart和ik_max_word。</p><p>ik_smart会将“清华大学”整个分为一个词，而ik_max_word会将“清华大学”分为“清华大学”，“清华”和“大学”，按需选其中之一就可以了。</p><p>修改默认分词方法(这里修改school_index索引的默认分词为：ik_max_word)：</p><div class="language-sql line-numbers-mode" data-ext="sql" data-title="sql"><pre class="language-sql"><code>PUT <span class="token operator">/</span>school_index
{
<span class="token string">&quot;settings&quot;</span> : {
<span class="token string">&quot;index&quot;</span> : {
<span class="token string">&quot;analysis.analyzer.default.type&quot;</span>: <span class="token string">&quot;ik_max_word&quot;</span>
}
}
}
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>分词器工作流程<br><img src="`+h+`" alt="img_117.png" loading="lazy"><br> 分词器由三部分组成：</p><ul><li>Character Filter：将文本中html标签剔除掉。</li><li>Tokenizer：按照规则进行分词，在英文中按照空格分词</li><li>Token Filter：将切分的单词进行加工，小写，删除 stopwords(停顿词，a、an、the、is等),增加同义词</li></ul><p>每个组件的作用，可参考下面的例子</p><p>character filter：在一段文本进行分词之前，先进行预处理，比如说最常见的就是，过滤html标签</p><p>tokenizer：分词，hello you and me --&gt; hello, you, and, me</p><p>token filter：lowercase，stop word，synonymom，liked --&gt; like，Tom --&gt; tom，a/the/an --&gt; 干掉，small --&gt; little</p><p>ES内置分词器</p><ul><li>Standard Analyzer - 默认分词器，按词切分，小写处理</li><li>Simple Analyzer - 按照非字母切分(符号被过滤), 小写处理</li><li>Stop Analyzer - 小写处理，停用词过滤(the,a,is)</li><li>Whitespace Analyzer - 按照空格切分，不转小写</li><li>Keyword Analyzer - 不分词，直接将输入当作输出</li><li>Patter Analyzer - 正则表达式，默认\\W+(非字符分割)</li><li>Language - 提供了30多种常见语言的分词器</li><li>Customer Analyzer 自定义分词器</li></ul><p>每个内置分词器的作用，可参考下面的例子</p><div class="language-sql line-numbers-mode" data-ext="sql" data-title="sql"><pre class="language-sql"><code><span class="token keyword">Set</span> the shape <span class="token keyword">to</span> semi<span class="token operator">-</span>transparent <span class="token keyword">by</span> calling set_trans<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>

standard analyzer：<span class="token keyword">set</span><span class="token punctuation">,</span> the<span class="token punctuation">,</span> shape<span class="token punctuation">,</span> <span class="token keyword">to</span><span class="token punctuation">,</span> semi<span class="token punctuation">,</span> transparent<span class="token punctuation">,</span> <span class="token keyword">by</span><span class="token punctuation">,</span> calling<span class="token punctuation">,</span> set_trans<span class="token punctuation">,</span> <span class="token number">5</span>（默认的是standard）

<span class="token keyword">simple</span> analyzer：<span class="token keyword">set</span><span class="token punctuation">,</span> the<span class="token punctuation">,</span> shape<span class="token punctuation">,</span> <span class="token keyword">to</span><span class="token punctuation">,</span> semi<span class="token punctuation">,</span> transparent<span class="token punctuation">,</span> <span class="token keyword">by</span><span class="token punctuation">,</span> calling<span class="token punctuation">,</span> <span class="token keyword">set</span><span class="token punctuation">,</span> trans

whitespace analyzer：<span class="token keyword">Set</span><span class="token punctuation">,</span> the<span class="token punctuation">,</span> shape<span class="token punctuation">,</span> <span class="token keyword">to</span><span class="token punctuation">,</span> semi<span class="token operator">-</span>transparent<span class="token punctuation">,</span> <span class="token keyword">by</span><span class="token punctuation">,</span> calling<span class="token punctuation">,</span> set_trans<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>

stop analyzer:移除停用词，比如a the it等等
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>定制分词器</p><div class="language-sql line-numbers-mode" data-ext="sql" data-title="sql"><pre class="language-sql"><code>PUT <span class="token operator">/</span>my_index
{
<span class="token string">&quot;settings&quot;</span>: {
<span class="token string">&quot;analysis&quot;</span>: {
<span class="token string">&quot;analyzer&quot;</span>: {
<span class="token string">&quot;es_std&quot;</span>: {
<span class="token string">&quot;type&quot;</span>: <span class="token string">&quot;standard&quot;</span><span class="token punctuation">,</span>
<span class="token string">&quot;stopwords&quot;</span>: <span class="token string">&quot;_english_&quot;</span>
}
}
}
}
}

GET <span class="token operator">/</span>my_index<span class="token operator">/</span>_analyze
{
<span class="token string">&quot;analyzer&quot;</span>: <span class="token string">&quot;standard&quot;</span><span class="token punctuation">,</span>
<span class="token string">&quot;text&quot;</span>: <span class="token string">&quot;a dog is in the house&quot;</span>
}

GET <span class="token operator">/</span>my_index<span class="token operator">/</span>_analyze
{
<span class="token string">&quot;analyzer&quot;</span>: <span class="token string">&quot;es_std&quot;</span><span class="token punctuation">,</span>
<span class="token string">&quot;text&quot;</span>:<span class="token string">&quot;a dog is in the house&quot;</span>
}

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>定制化自己的分词器</p><div class="language-sql line-numbers-mode" data-ext="sql" data-title="sql"><pre class="language-sql"><code>PUT <span class="token operator">/</span>my_index
{
<span class="token string">&quot;settings&quot;</span>: {
<span class="token string">&quot;analysis&quot;</span>: {
<span class="token string">&quot;char_filter&quot;</span>: {
<span class="token string">&quot;&amp;_to_and&quot;</span>: {
<span class="token string">&quot;type&quot;</span>: <span class="token string">&quot;mapping&quot;</span><span class="token punctuation">,</span>
<span class="token string">&quot;mappings&quot;</span>: <span class="token punctuation">[</span><span class="token string">&quot;&amp;=&gt; and&quot;</span><span class="token punctuation">]</span>
}
}<span class="token punctuation">,</span>
<span class="token string">&quot;filter&quot;</span>: {
<span class="token string">&quot;my_stopwords&quot;</span>: {
<span class="token string">&quot;type&quot;</span>: <span class="token string">&quot;stop&quot;</span><span class="token punctuation">,</span>
<span class="token string">&quot;stopwords&quot;</span>: <span class="token punctuation">[</span><span class="token string">&quot;the&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;a&quot;</span><span class="token punctuation">]</span>
}
}<span class="token punctuation">,</span>
<span class="token string">&quot;analyzer&quot;</span>: {
<span class="token string">&quot;my_analyzer&quot;</span>: {
<span class="token string">&quot;type&quot;</span>: <span class="token string">&quot;custom&quot;</span><span class="token punctuation">,</span>
<span class="token string">&quot;char_filter&quot;</span>: <span class="token punctuation">[</span><span class="token string">&quot;html_strip&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;&amp;_to_and&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token string">&quot;tokenizer&quot;</span>: <span class="token string">&quot;standard&quot;</span><span class="token punctuation">,</span>
<span class="token string">&quot;filter&quot;</span>: <span class="token punctuation">[</span><span class="token string">&quot;lowercase&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;my_stopwords&quot;</span><span class="token punctuation">]</span>
}
}
}
}
}

GET <span class="token operator">/</span>my_index<span class="token operator">/</span>_analyze
{
<span class="token string">&quot;text&quot;</span>: <span class="token string">&quot;tom&amp;jerry are a friend in the house, &lt;a&gt;, HAHA!!&quot;</span><span class="token punctuation">,</span>
<span class="token string">&quot;analyzer&quot;</span>: <span class="token string">&quot;my_analyzer&quot;</span>
}

PUT <span class="token operator">/</span>my_index<span class="token operator">/</span>_mapping<span class="token operator">/</span>my_type
{
<span class="token string">&quot;properties&quot;</span>: {
<span class="token string">&quot;content&quot;</span>: {
<span class="token string">&quot;type&quot;</span>: <span class="token string">&quot;text&quot;</span><span class="token punctuation">,</span>
<span class="token string">&quot;analyzer&quot;</span>: <span class="token string">&quot;my_analyzer&quot;</span>
}
}
}

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>IK热更新</p><p>每次都是在es的扩展词典中，手动添加新词语，很坑</p><p>（1）每次添加完，都要重启es才能生效，非常麻烦</p><p>（2）es是分布式的，可能有数百个节点，你不能每次都一个一个节点上面去修改</p><p>es不停机，直接我们在外部某个地方添加新的词语，es中立即热加载到这些新词语<br> IKAnalyzer.cfg.xml</p><div class="language-sql line-numbers-mode" data-ext="sql" data-title="sql"><pre class="language-sql"><code><span class="token operator">&lt;</span>properties<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span><span class="token keyword">comment</span><span class="token operator">&gt;</span>IK Analyzer 扩展配置<span class="token operator">&lt;</span><span class="token operator">/</span><span class="token keyword">comment</span><span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span><span class="token operator">!</span><span class="token comment">--用户可以在这里配置自己的扩展字典 --&gt;</span>
	<span class="token operator">&lt;</span>entry <span class="token keyword">key</span><span class="token operator">=</span><span class="token string">&quot;ext_dict&quot;</span><span class="token operator">&gt;</span>location<span class="token operator">&lt;</span><span class="token operator">/</span>entry<span class="token operator">&gt;</span>
	 <span class="token operator">&lt;</span><span class="token operator">!</span><span class="token comment">--用户可以在这里配置自己的扩展停止词字典--&gt;</span>
	<span class="token operator">&lt;</span>entry <span class="token keyword">key</span><span class="token operator">=</span><span class="token string">&quot;ext_stopwords&quot;</span><span class="token operator">&gt;</span>location<span class="token operator">&lt;</span><span class="token operator">/</span>entry<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span><span class="token operator">!</span><span class="token comment">--用户可以在这里配置远程扩展字典 --&gt;</span>
	<span class="token operator">&lt;</span>entry <span class="token keyword">key</span><span class="token operator">=</span><span class="token string">&quot;remote_ext_dict&quot;</span><span class="token operator">&gt;</span>words_location<span class="token operator">&lt;</span><span class="token operator">/</span>entry<span class="token operator">&gt;</span> 
	<span class="token operator">&lt;</span><span class="token operator">!</span><span class="token comment">--用户可以在这里配置远程扩展停止词字典--&gt;</span>
	<span class="token operator">&lt;</span>entry <span class="token keyword">key</span><span class="token operator">=</span><span class="token string">&quot;remote_ext_stopwords&quot;</span><span class="token operator">&gt;</span>words_location<span class="token operator">&lt;</span><span class="token operator">/</span>entry<span class="token operator">&gt;</span>
<span class="token operator">&lt;</span><span class="token operator">/</span>properties<span class="token operator">&gt;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_7-es评分机制" tabindex="-1"><a class="header-anchor" href="#_7-es评分机制"><span>7.es评分机制</span></a></h3><p>当你通过关键字搜索相关文档时，可能会出现多个文档，这些文档的顺序是通过一个max_score属性的大小从高到低顺序展现出来的，max_score属性就是我们所说的评分。</p><p>打分算法</p><p>relevance score算法，简单来说，就是计算出，一个索引中的文本，与搜索文本，他们之间的关联匹配程度</p><p>Elasticsearch使用的是 term frequency/inverse document frequency算法，简称为TF/IDF算法</p><p>总公式</p><p>max_score = boost * idf * tf</p><figure><img src="`+k+'" alt="img_118.png" tabindex="0" loading="lazy"><figcaption>img_118.png</figcaption></figure><p>对于查询权重我们可以自己定义</p><p>如下面所示：</p><figure><img src="'+b+`" alt="img_119.png" tabindex="0" loading="lazy"><figcaption>img_119.png</figcaption></figure><p>TF算法</p><p>Term frequency：搜索文本中的各个词条在field文本中出现了多少次，出现次数越多，就越相关</p><ul><li>搜索请求：hello world</li><li>doc1：hello you, and world is very good</li><li>doc2：hello, how are you</li></ul><p>doc1这个文档匹配了2个单词，所以doc1的得分要高一些。</p><p>2.IDF算法<br> Inverse document frequency：搜索文本中的各个词条在整个索引的所有文档中出现了多少次，出现的次数越多，就越不相关</p><ul><li>搜索请求：hello world</li><li>doc1：hello, tuling is very good</li><li>doc2：hi world, how are you</li></ul><p>比如说，在index中有1万条document，hello这个单词在所有的document中，一共出现了1000次；world这个单词在所有的document中，一共出现了100次，所以world这个单词得分就更高。</p><p>那么匹配world的doc得分就越高,越有可能排在搜索结果前面。</p><p>Field-length norm：field长度，field越长，相关度越弱<br> 搜索请求：hello world</p><div class="language-xml line-numbers-mode" data-ext="xml" data-title="xml"><pre class="language-xml"><code>doc1：{ &quot;title&quot;: &quot;hello article&quot;, &quot;content&quot;: &quot;...... N个单词&quot; }
doc2：{ &quot;title&quot;: &quot;my article&quot;, &quot;content&quot;: &quot;...... N个单词，hi world&quot; }

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>hello world在整个index中出现的次数是一样多的，那么doc1更相关，title field更短</p><p><strong>2.空间向量模型（vector space model）</strong></p><p>每个doc vector计算出对query vector的弧度，最后基于这个弧度给出一个doc相对于query中多个term的总分数</p><p>弧度越大，分数月底; 弧度越小，分数越高</p><p>如果是多个term，那么就是线性代数来计算，无法用图表示</p><figure><img src="`+q+'" alt="img_120.png" tabindex="0" loading="lazy"><figcaption>img_120.png</figcaption></figure><p>了解即可，掌握对应的打分算法我自己认为就足够了，不用砖牛角尖</p><h2 id="三、elasticsearch架构设计" tabindex="-1"><a class="header-anchor" href="#三、elasticsearch架构设计"><span>三、ElasticSearch架构设计</span></a></h2><p>es集群架构</p><p>如下图 ，就是一个三个节点组成的es集群，p0、p1、p2表示一个节点中的分片，R0、R1、R2表示分片对应的副本<br><img src="'+_+'" alt="img_121.png" loading="lazy"><br> 在Elasticsearch主要分成两类节点，一类是Master，一类是DataNode。</p><h3 id="_1-master节点" tabindex="-1"><a class="header-anchor" href="#_1-master节点"><span>1.Master节点</span></a></h3><p>在Elasticsearch启动时，会选举出来一个Master节点。</p><p>当某个节点启动后，然后使用Zen Discovery机制找到集群中的其他节点，并建立连接。</p><p>discovery.seed_hosts: [“192.168.21.130”, “192.168.21.131”, “192.168.21.132”]</p><p>并从候选主节点中选举出一个主节点。</p><p>cluster.initial_master_nodes: [“node1”, “node2”,“node3”]</p><p>Master节点主要负责</p><ul><li>管理索引（创建索引、删除索引）、分配分片</li><li>维护元数据</li><li>管理集群节点状态</li><li>不负责数据写入和查询，比较轻量级</li></ul><p>一个Elasticsearch集群中，只有一个Master节点。在生产环境中，内存可以相对小一点，但机器要稳定。</p><h3 id="_2-datanode节点" tabindex="-1"><a class="header-anchor" href="#_2-datanode节点"><span>2.DataNode节点</span></a></h3><p>在Elasticsearch集群中，会有N个DataNode节点。</p><p>DataNode节点主要负责：</p><p>数据写入、数据检索，</p><p>大部分Elasticsearch的压力都在DataNode节点上在生产环境中，内存最好配置大一些</p><h3 id="_3-分片" tabindex="-1"><a class="header-anchor" href="#_3-分片"><span>3.分片</span></a></h3><p>Elasticsearch是一个分布式的搜索引擎，索引的数据也是分成若干部分，分布在不同的服务器节点中，分布在不同服务器节点中的索引数据，就是分片（Shard）。</p><p>Elasticsearch会自动管理分片，如果发现分片分布不均衡，就会自动迁移一个索引（index）由多个shard（分片）组成，而分片是分布在不同的服务器上的.</p><h3 id="_4-副本" tabindex="-1"><a class="header-anchor" href="#_4-副本"><span>4.副本</span></a></h3><p>为了对Elasticsearch的分片进行容错，假设某个节点不可用，会导致整个索引库都将不可用。所以，需要对分片进行副本容错，每一个分片都会有对应的副本。</p><p>在Elasticsearch中，默认创建的索引为1个分片、每个分片有1个主分片和1个副本分片。</p><p>每个分片都会有一个Primary Shard（主分片），也会有若干个Replica Shard（副本分片）</p><p>Primary Shard和Replica Shard不在同一个节点上</p><h3 id="_5-es集群读写流程" tabindex="-1"><a class="header-anchor" href="#_5-es集群读写流程"><span>5.es集群读写流程</span></a></h3><p>es 写数据流程</p><figure><img src="'+f+'" alt="img_122.png" tabindex="0" loading="lazy"><figcaption>img_122.png</figcaption></figure><p>1.选择任意一个DataNode发送请求，例如：node2。此时，node2就成为一个coordinating node（协调节点）</p><p>2.计算得到文档要写入的分片</p><p><code>shard = hash(routing) % number_of_primary_shards</code><br> routing 是一个可变值，默认是文档的 _id</p><p>3.coordinating node会进行路由，将请求转发给对应的primary shard所在的DataNode（假设primary shard在node1、replica shard在node2）</p><p>4.node1节点上的Primary Shard处理请求，写入数据到索引库中，并将数据同步到Replica shard</p><p>5.Primary Shard和Replica Shard都保存好了文档，返回client</p><p>es 读数据流程</p><figure><img src="'+y+'" alt="img_123.png" tabindex="0" loading="lazy"><figcaption>img_123.png</figcaption></figure><p>1.client发起查询请求，某个DataNode接收到请求，该DataNode就会成为协调节点（Coordinating Node）</p><p>2.协调节点（Coordinating Node）将查询请求广播到每一个数据节点，这些数据节点的分片会处理该查询请求</p><p>3.每个分片进行数据查询，将符合条件的数据放在一个优先队列中，并将这些数据的文档ID、节点信息、分片信息返回给协调节点</p><p>4.协调节点将所有的结果进行汇总，并进行全局排序</p><p>5.协调节点向包含这些文档ID的分片发送get请求，对应的分片将文档数据返回给协调节点，最后协调节点将数据返回给客户端</p><p>注意：写请求是写入 primary shard，然后同步给所有的 replica shard；读请求可以从 primary shard 或 replica shard 读取，采用的是随机轮询算法。</p><p>自己总结：es的检索流程和mysql数据表查询非主键索引的思路有些相似，先从索引表查询出对应的主键索引值，在进行回表查询具体的行数据。</p><p>es删除/更新数据底层原理</p><p>如果是删除操作，commit 的时候会生成一个 .del 文件，里面将某个 doc 标识为 deleted 状态，那么搜索的时候根据 .del 文件就知道这个 doc 是否被删除了。</p><p>如果是更新操作，就是将原来的 doc 标识为 deleted 状态，然后新写入一条数据。</p><p>es 底层写数据原理</p><figure><img src="'+x+'" alt="img_124.png" tabindex="0" loading="lazy"><figcaption>img_124.png</figcaption></figure><p>简述：先写入内存 buffer，在 buffer 里的时候数据是搜索不到的；然后刷新到os cache中（同时将数据备份到translog日志文件），最后刷新到 segment file磁盘文件中</p><p>总结一下，数据先写入内存 buffer，然后每隔 1s，将数据 refresh 到 os cache，到了 os cache 数据就能被搜索到（所以我们才说 es 从写入到能被搜索到，中间有 1s 的延迟）。每隔 5s，将数据写入 translog 文件作备份（这样如果机器宕机，内存数据全没，最多会有 5s 的数据丢失），translog 大到一定程度，或者默认每隔 30mins，会触发 commit 操作，将缓冲区的数据都 flush 到 segment file 磁盘文件中。最后存入到commit point磁盘文件中。</p><p>refresh到文件系统缓存</p><p>当数据写入到ES分片时，会首先写入到内存中，然后通过内存的buffer生成一个segment，并刷到文件系统缓存中。</p><p>如果 buffer 快满了，或者到一定时间，就会将内存 buffer 数据 refresh 到一个新的 segment file中，但是此时数据不是直接进入 segment file 磁盘文件，而是先进入 os cache 。</p><p>这个过程就是 refresh 。</p><p>只要 buffer 中的数据被 refresh 操作刷入 os cache 中，这个数据就可以被搜索到了。</p><p>定时 refresh到文件系统缓存机制</p><p>每隔 1 秒钟，es 将 buffer 中的数据写入一个新的 segment file ，每秒钟会产生一个新的磁盘文件 segment file ，这个 segment file 中就存储最近 1 秒内 buffer 中写入的数据。<br> 但是如果 buffer 里面此时没有数据，那当然不会执行 refresh 操作，如果 buffer 里面有数据，默认 1 秒钟执行一次 refresh 操作，刷入一个新的 segment file 中。</p><p>segment file合并</p><p>buffer 每 refresh 一次，就会产生一个 segment file ，所以默认情况下是 1 秒钟一个 segment file ，这样下来 segment file 会越来越多，此时会定期执行 merge，减少索引查询时IO开销。<br> 每次 merge 的时候，会将多个 segment file 合并成一个，同时这里会将标识为 deleted 的 doc 给物理删除掉（之前执行过的delete的数据），然后将新的 segment file 写入到一个 commit point磁盘 。</p><p>知识扩展</p><p>操作系统里面，磁盘文件其实都有一个东西，叫做 os cache ，即操作系统缓存，就是说数据写入磁盘文件之前，会先进入 os cache ，先进入操作系统级别的一个内存缓存中去。</p><p>备份到translog磁盘</p><p>刷新到translog文件以保障数据不丢失，translog的设计思想和mysql的redo log是相似的。</p><p>每隔5s,从os cache 中同步到translog磁盘里面去做备份。</p><p>那么translog 日志文件的作用是什么？</p><p>你执行 commit 操作之前，数据要么是停留在 buffer 中，要么是停留在 os cache 中，无论是 buffer 还是 os cache 都是内存，一旦这台机器死了，内存中的数据就全丢了。所以需要将数据对应的操作写入一个专门的日志文件 translog 中，一旦此时机器宕机，再次重启的时候，es 会自动读取 translog 日志文件中的数据，恢复到内存 buffer 和 os cache 中去。</p><p>flush到磁盘文件</p><p>重复上面的步骤，新的数据不断进入 buffer 和 translog，不断将 buffer 数据写入一个又一个新的 segment file文件系统缓存中去，每次 refresh 完 buffer 清空，translog 保留。随着这个过程推进，translog 会变得越来越大。当 translog 达到一定长度的时候，就会触发 commit 操作。</p><p>数据最终被flush到磁盘文件就完成了数据的最终归宿。</p><figure><img src="'+E+'" alt="img_125.png" tabindex="0" loading="lazy"><figcaption>img_125.png</figcaption></figure><p>commit操作流程</p><p>commit 操作首先就是将 buffer 中现有数据 refresh 到 os cache 中去，清空 buffer。<br> 然后，将一个 commit point 写入磁盘文件，里面标识着这个 commit point 对应的所有 segment file ，同时强行将 os cache 中目前所有的数据都 fsync 到磁盘文件中去。<br> 最后清空 现有 translog 日志文件，重启一个 translog，此时 commit 操作完成。</p><p>这个 commit 操作叫做 flush 。</p><p>默认 30 分钟自动执行一次 flush ，将文件系统缓存的数据刷入到磁盘。<br> 但如果 translog 过大，也会触发 flush 。</p><p>flush 操作就对应着 commit 的全过程，我们可以通过 es api，手动执行 flush 操作，手动将 os cache 中的数据 fsync 强刷到磁盘上去。</p><p>es准实时机制</p><p>为什么叫 es 是准实时的？</p><p>NRT ，全称 near real-time 。默认是每隔 1 秒 refresh 一次的，所以 es 是准实时的，因为写入的数据 1 秒之后才能被看到。</p><p>可以通过 es 的 restful api 或者 java api ，手动执行一次 refresh 操作，就是手动将 buffer 中的数据刷入 os cache 中，让数据立马就可以被搜索到。只要数据被输入 os cache 中，buffer 就会被清空了，因为不需要保留 buffer 了，数据在 translog 里面已经持久化到磁盘去一份了。</p><p>es会数据丢失吗？</p><p>可能会丢失有 5 秒的数据，停留在 buffer、translog os cache、segment file os cache 中，而不在磁盘上，此时如果备份到translog过程中宕机，会导致 5 秒的数据丢失。</p><p>translog 其实也是先写入 os cache 的，默认每隔 5 秒刷一次到磁盘中去，所以默认情况下，可能有 5 秒的数据会仅仅停留在 buffer 或者 translog 文件的 os cache 中，如果此时机器挂了，会丢失 5 秒钟的数据。但是这样性能比较好，最多丢 5 秒的数据。<br> 也可以将 translog 设置成每次写操作必须是直接 fsync 到磁盘，但是性能会差很多。</p><p>es集群脑裂</p><p>关于集群脑裂的定义请参考我的另一篇博文</p>',245)),n("p",null,[n("a",T,[s[0]||(s[0]=a("https://blog.csdn.net/sinat_34814635/article/details/129914369")),i(e)])]),s[5]||(s[5]=t('<p>那么es是如何解决脑裂问题的？</p><p>es是直到有足够的master候选节点时，才可以选举出一个master，否则就不要选举出一个master。</p><p>这个参数必须被设置为集群中master候选节点的quorum数量，也就是大多数，至于quorum的算法，就是：master候选节点数量 / 2 + 1。</p><p>综上所述，一个生产环境的es集群，至少要有3个节点，同时将这个参数设置为quorum，也就是2。discovery.zen.minimum_master_nodes设置为2</p><p>那么这个是参数是如何避免脑裂问题的产生的呢？比如我们有3个节点，quorum是2.现在网络故障，1个节点在一个网络区域，另外2个节点在另外一个网络区域，不同的网络区域内无法通信。</p><p>这个时候有两种情况情况：</p><p>（1）如果master是单独的那个节点，另外2个节点是master候选节点，那么此时那个单独的master节点因为没有指定数量的候选master node在自己当前所在的集群内，因此就会取消当前master的角色，尝试重新选举，但是无法选举成功。然后另外一个网络区域内的node因为无法连接到master，就会发起重新选举，因为有两个master候选节点，满足了quorum，因此可以成功选举出一个master。此时集群中就会还是只有一个master。</p><p>（2）如果master和另外一个node在一个网络区域内，然后一个node单独在一个网络区域内。那么此时那个单独的node因为连接不上master，会尝试发起选举，但是因为master候选节点数量不到quorum，因此无法选举出master。而另外一个网络区域内，原先的那个master还会继续工作。这也可以保证集群内只有一个master节点。</p><p>综上所述，集群中master节点的数量至少3台，三台主节点通过在elasticsearch.yml中配置discovery.zen.minimum_master_nodes: 2，就可以避免脑裂问题的产生。</p><h2 id="四、elasticsearch应用" tabindex="-1"><a class="header-anchor" href="#四、elasticsearch应用"><span>四、ElasticSearch应用</span></a></h2><h3 id="_1-elasticsearch-sql" tabindex="-1"><a class="header-anchor" href="#_1-elasticsearch-sql"><span>1.Elasticsearch SQL</span></a></h3><figure><img src="'+w+'" alt="img_126.png" tabindex="0" loading="lazy"><figcaption>img_126.png</figcaption></figure><p>Elasticsearch SQL允许执行类SQL的查询，可以使用REST接口、命令行或者是JDBC，都可以使用SQL来进行数据的检索和数据的聚合。</p><p>Elasticsearch SQL特点：</p><p>本地集成</p><p>Elasticsearch SQL是专门为Elasticsearch构建的。每个SQL查询都根据底层存储对相关节点有效执行。</p><p>没有额外的要求</p><p>不依赖其他的硬件、进程、运行时库，Elasticsearch SQL可以直接运行在Elasticsearch集群上</p><p>轻量且高效</p><p>像SQL那样简洁、高效地完成查询</p><p>Elasticsearch SQL提供了sql转换的功能，但是只能满足一些简单的查询，例如：不支持JOIN、不支持较复杂的子查询。官方还是推荐使用DSL语句来实现</p><h3 id="_2-java客户端" tabindex="-1"><a class="header-anchor" href="#_2-java客户端"><span>2.Java客户端</span></a></h3><p>这个就自己看官网吧</p><h3 id="_3-es和mysql的双写" tabindex="-1"><a class="header-anchor" href="#_3-es和mysql的双写"><span>3.es和mysql的双写</span></a></h3><p>双写的流程：</p><ul><li>1.先写数据库，然后写es，es只存常搜索的索引字段。</li><li>2.读取时先读es，找到对应主键后，然后根据主键在读mysql。（这种场景主要出现在mysql是分表的，而主键配置的是分片键，非分片键的查询将扫描全表）<br> 首先不建议进行双写，因为会造成数据不一致这样新的问题。造成不一致的原因为写入es或者更新es字段失败，加上补充机制后也没有成功。</li></ul><p>双写的同步</p><p>分页查询mysql中的数据，放入mq中（放入mq中的原因，为读取mysql的线程会远比执行同步的线程速度快），同步线程从mq中拉起数据，然后和es对比，如果不同，与mysql中的数据为准，进行删除复制。</p><p>补偿机制</p><p>我认为任何对es写失败，更新失败，删除失败都应该纪录下来，然后去手动操作使其同步。</p><h2 id="五、elasticsearch安装配置" tabindex="-1"><a class="header-anchor" href="#五、elasticsearch安装配置"><span>五、ElasticSearch安装配置</span></a></h2><h3 id="_1-安装elasticsearch" tabindex="-1"><a class="header-anchor" href="#_1-安装elasticsearch"><span>1.安装ElasticSearch</span></a></h3><p>解压下载的压缩包，本次使用7.14.0版本</p><p>从v7开始，elasticsearch不用单独安装JDK，因为它在下载时会自动下载对应的jdk包。因此不用额外下载jdk和配置环境变量。</p><p>在终端cd到elasticsearch的bin目录，运行命令./elasticsearch即可开启es数据库服务（在终端通过按control + c可停止服务）。在网页中访问localhost:9200看到json结果即启动成功。</p><p>如下图所示：</p><figure><img src="'+S+'" alt="img_127.png" tabindex="0" loading="lazy"><figcaption>img_127.png</figcaption></figure><h3 id="_2-安装kibana" tabindex="-1"><a class="header-anchor" href="#_2-安装kibana"><span>2.安装Kibana</span></a></h3><p>下载可视化工具Kibanahttps://www.elastic.co/cn/downloads/kibana</p><p>解压下载的压缩包，并将其复制粘贴至自己想要存放的目录。Kibana的版本最好和es保持一致，如这次同样使用7.14.0版本</p><p>启动Kibana</p><p>在终端cd到Kibana目录下，运行命令./Kibana即可开启Kibana的端口访问。在网页中访问http://localhost:5601跳转到如下界面即访问成功。</p>',42)),n("p",null,[s[2]||(s[2]=a("访问下面地址：")),n("a",L,[s[1]||(s[1]=a("http://localhost:5601/app/dev_tools#/console")),i(e)]),s[3]||(s[3]=a(" 可进入管理后台"))]),s[6]||(s[6]=n("figure",null,[n("img",{src:z,alt:"img_128.png",tabindex:"0",loading:"lazy"}),n("figcaption",null,"img_128.png")],-1)),s[7]||(s[7]=n("p",null,"ElasticSearch可视化工具之cerebro",-1))])}const I=l(D,[["render",N],["__file","elasticsearch.html.vue"]]),Q=JSON.parse('{"path":"/assembly/elasticsearch.html","title":"组件ElasticSearch","lang":"zh-CN","frontmatter":{"title":"组件ElasticSearch","icon":"laptop-code","category":["设计组件"],"tag":["组件"],"description":"一、 简介 Elasticsearch 是一个分布式可扩展的实时搜索和分析引擎,一个建立在全文搜索引擎 Apache Lucene™ 基础上的搜索引擎.当然 Elasticsearch 并不仅仅是 Lucene 那么简单，它不仅包括了全文搜索功能，还可以进行以下工作: 一个分布式的实时文档存储，每个字段可以被索引与搜索 一个分布式实时分析搜索引擎 能胜...","head":[["meta",{"property":"og:url","content":"https://ujava.cn/assembly/elasticsearch.html"}],["meta",{"property":"og:site_name","content":"UJava"}],["meta",{"property":"og:title","content":"组件ElasticSearch"}],["meta",{"property":"og:description","content":"一、 简介 Elasticsearch 是一个分布式可扩展的实时搜索和分析引擎,一个建立在全文搜索引擎 Apache Lucene™ 基础上的搜索引擎.当然 Elasticsearch 并不仅仅是 Lucene 那么简单，它不仅包括了全文搜索功能，还可以进行以下工作: 一个分布式的实时文档存储，每个字段可以被索引与搜索 一个分布式实时分析搜索引擎 能胜..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-04-15T03:42:50.000Z"}],["meta",{"property":"article:author","content":"UJava"}],["meta",{"property":"article:tag","content":"组件"}],["meta",{"property":"article:modified_time","content":"2024-04-15T03:42:50.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"组件ElasticSearch\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2024-04-15T03:42:50.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"UJava\\",\\"url\\":\\"https://ujava.cn/article/\\"}]}"]]},"headers":[{"level":2,"title":"一、 简介","slug":"一、-简介","link":"#一、-简介","children":[{"level":3,"title":"1.ES优点","slug":"_1-es优点","link":"#_1-es优点","children":[]},{"level":3,"title":"2.ES缺点","slug":"_2-es缺点","link":"#_2-es缺点","children":[]},{"level":3,"title":"3.ES使用场景","slug":"_3-es使用场景","link":"#_3-es使用场景","children":[]},{"level":3,"title":"4.DSL语言高级查询","slug":"_4-dsl语言高级查询","link":"#_4-dsl语言高级查询","children":[]},{"level":3,"title":"5.文档映射","slug":"_5-文档映射","link":"#_5-文档映射","children":[]},{"level":3,"title":"6.suggest search","slug":"_6-suggest-search","link":"#_6-suggest-search","children":[]}]},{"level":2,"title":"二、ES核心原理","slug":"二、es核心原理","link":"#二、es核心原理","children":[{"level":3,"title":"1.索引（Index）","slug":"_1-索引-index","link":"#_1-索引-index","children":[]},{"level":3,"title":"2.文档类型（Type）","slug":"_2-文档类型-type","link":"#_2-文档类型-type","children":[]},{"level":3,"title":"3.文档（Document）","slug":"_3-文档-document","link":"#_3-文档-document","children":[]},{"level":3,"title":"4.字段（Field）","slug":"_4-字段-field","link":"#_4-字段-field","children":[]},{"level":3,"title":"5.倒排索引","slug":"_5-倒排索引","link":"#_5-倒排索引","children":[]},{"level":3,"title":"6.分词器","slug":"_6-分词器","link":"#_6-分词器","children":[]},{"level":3,"title":"7.es评分机制","slug":"_7-es评分机制","link":"#_7-es评分机制","children":[]}]},{"level":2,"title":"三、ElasticSearch架构设计","slug":"三、elasticsearch架构设计","link":"#三、elasticsearch架构设计","children":[{"level":3,"title":"1.Master节点","slug":"_1-master节点","link":"#_1-master节点","children":[]},{"level":3,"title":"2.DataNode节点","slug":"_2-datanode节点","link":"#_2-datanode节点","children":[]},{"level":3,"title":"3.分片","slug":"_3-分片","link":"#_3-分片","children":[]},{"level":3,"title":"4.副本","slug":"_4-副本","link":"#_4-副本","children":[]},{"level":3,"title":"5.es集群读写流程","slug":"_5-es集群读写流程","link":"#_5-es集群读写流程","children":[]}]},{"level":2,"title":"四、ElasticSearch应用","slug":"四、elasticsearch应用","link":"#四、elasticsearch应用","children":[{"level":3,"title":"1.Elasticsearch SQL","slug":"_1-elasticsearch-sql","link":"#_1-elasticsearch-sql","children":[]},{"level":3,"title":"2.Java客户端","slug":"_2-java客户端","link":"#_2-java客户端","children":[]},{"level":3,"title":"3.es和mysql的双写","slug":"_3-es和mysql的双写","link":"#_3-es和mysql的双写","children":[]}]},{"level":2,"title":"五、ElasticSearch安装配置","slug":"五、elasticsearch安装配置","link":"#五、elasticsearch安装配置","children":[{"level":3,"title":"1.安装ElasticSearch","slug":"_1-安装elasticsearch","link":"#_1-安装elasticsearch","children":[]},{"level":3,"title":"2.安装Kibana","slug":"_2-安装kibana","link":"#_2-安装kibana","children":[]}]}],"git":{"createdTime":1713151551000,"updatedTime":1713152570000,"contributors":[{"name":"yangchunjian","email":"1091938307@qq.com","commits":2}]},"readingTime":{"minutes":28.01,"words":8402},"filePathRelative":"assembly/elasticsearch.md","localizedDate":"2024年4月15日","excerpt":"<h2>一、 简介</h2>\\n<p>Elasticsearch 是一个分布式可扩展的实时搜索和分析引擎,一个建立在全文搜索引擎 Apache Lucene™ 基础上的搜索引擎.当然 Elasticsearch 并不仅仅是 Lucene 那么简单，它不仅包括了全文搜索功能，还可以进行以下工作:</p>\\n<ul>\\n<li>一个分布式的实时文档存储，每个字段可以被索引与搜索</li>\\n<li>一个分布式实时分析搜索引擎</li>\\n<li>能胜任上百个服务节点的扩展，并支持 PB 级别的结构化或者非结构化数据</li>\\n</ul>\\n<h3>1.ES优点</h3>\\n<ul>\\n<li>1.支持全文搜索，并基于倒排索引，检索速度特别快</li>\\n<li>2.高可用性，支持集群部署，任何节点失效系统自动调整。</li>\\n</ul>","autoDesc":true}');export{I as comp,Q as data};
