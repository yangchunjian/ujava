const e=JSON.parse(`{"key":"v-99558f76","path":"/base/optialgorithm.html","title":"优化算法","lang":"zh-CN","frontmatter":{"title":"优化算法","icon":"laptop-code","category":["设计基础"],"tag":["基础"],"description":"最优化方法是一种数学方法，它是研究在给定约束之下如何寻求某些因素(的量)，以使某一(或某些)指标达到最优的一些学科的总称。在学习机器学习的过程中我们发现，大部分的机器学习算法的本质都是建立优化模型，通过最优化方法对目标函数（或损失函数）进行优化，从而训练出最好的模型。常见的最优化方法有梯度下降法、牛顿法和拟牛顿法、共轭梯度法、拉格朗日数乘法等等。 1. 梯度下降法（Gradient Descent） 梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。**梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢。**梯度下降法的搜索迭代示意图如下图所示：","head":[["meta",{"property":"og:url","content":"https://ujava.cn/base/optialgorithm.html"}],["meta",{"property":"og:site_name","content":"UJava"}],["meta",{"property":"og:title","content":"优化算法"}],["meta",{"property":"og:description","content":"最优化方法是一种数学方法，它是研究在给定约束之下如何寻求某些因素(的量)，以使某一(或某些)指标达到最优的一些学科的总称。在学习机器学习的过程中我们发现，大部分的机器学习算法的本质都是建立优化模型，通过最优化方法对目标函数（或损失函数）进行优化，从而训练出最好的模型。常见的最优化方法有梯度下降法、牛顿法和拟牛顿法、共轭梯度法、拉格朗日数乘法等等。 1. 梯度下降法（Gradient Descent） 梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。**梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢。**梯度下降法的搜索迭代示意图如下图所示："}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-04-27T15:25:14.000Z"}],["meta",{"property":"article:author","content":"UJava"}],["meta",{"property":"article:tag","content":"基础"}],["meta",{"property":"article:modified_time","content":"2024-04-27T15:25:14.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"优化算法\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2024-04-27T15:25:14.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"UJava\\",\\"url\\":\\"https://ujava.cn/article/\\"}]}"]]},"headers":[{"level":3,"title":"1. 梯度下降法（Gradient Descent）","slug":"_1-梯度下降法-gradient-descent","link":"#_1-梯度下降法-gradient-descent","children":[]},{"level":3,"title":"2. 牛顿法和拟牛顿法（Newton's method & Quasi-Newton Methods）","slug":"_2-牛顿法和拟牛顿法-newton-s-method-quasi-newton-methods","link":"#_2-牛顿法和拟牛顿法-newton-s-method-quasi-newton-methods","children":[]},{"level":2,"title":"具体步骤：","slug":"具体步骤","link":"#具体步骤","children":[{"level":3,"title":"3. 共轭梯度法（Conjugate Gradient）","slug":"_3-共轭梯度法-conjugate-gradient","link":"#_3-共轭梯度法-conjugate-gradient","children":[]},{"level":3,"title":"4. 启发式优化方法","slug":"_4-启发式优化方法","link":"#_4-启发式优化方法","children":[]},{"level":3,"title":"5. 解决约束优化问题——拉格朗日乘数法","slug":"_5-解决约束优化问题——拉格朗日乘数法","link":"#_5-解决约束优化问题——拉格朗日乘数法","children":[]},{"level":3,"title":"5.1 拉格朗日乘数法的基本思想","slug":"_5-1-拉格朗日乘数法的基本思想","link":"#_5-1-拉格朗日乘数法的基本思想","children":[]},{"level":3,"title":"5.2 数学实例","slug":"_5-2-数学实例","link":"#_5-2-数学实例","children":[]},{"level":3,"title":"5.3 拉格朗日乘数法的基本形态","slug":"_5-3-拉格朗日乘数法的基本形态","link":"#_5-3-拉格朗日乘数法的基本形态","children":[]},{"level":3,"title":"5.4 拉格朗日乘数法与KKT条件","slug":"_5-4-拉格朗日乘数法与kkt条件","link":"#_5-4-拉格朗日乘数法与kkt条件","children":[]}]}],"git":{"createdTime":1714231514000,"updatedTime":1714231514000,"contributors":[{"name":"yangchunjian","email":"1091938307@qq.com","commits":1}]},"readingTime":{"minutes":18.88,"words":5664},"filePathRelative":"base/optialgorithm.md","localizedDate":"2024年4月27日","excerpt":"<p>最优化方法是一种数学方法，它是研究在给定约束之下如何寻求某些因素(的量)，以使某一(或某些)指标达到最优的一些学科的总称。在学习机器学习的过程中我们发现，大部分的机器学习算法的本质都是建立优化模型，通过最优化方法对目标函数（或损失函数）进行优化，从而训练出最好的模型。常见的最优化方法有梯度下降法、牛顿法和拟牛顿法、共轭梯度法、拉格朗日数乘法等等。</p>\\n<h3> 1. 梯度下降法（Gradient Descent）</h3>\\n<p>梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。**梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢。**梯度下降法的搜索迭代示意图如下图所示：<br>\\n</p>","copyright":{"author":"UJava(ujava.cn)","license":"MIT"},"autoDesc":true}`);export{e as data};
